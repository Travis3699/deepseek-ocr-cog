"""
DeepSeek-OCR on Replicate Cog
æ”¯æŒåœ–ç‰‡ï¼ˆJPG/PNG/WebPï¼‰+ PDF
"""

from cog import BasePredictor, Input
import torch
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import requests
from io import BytesIO
import tempfile
import os


class Predictor(BasePredictor):
    """Replicate é æ¸¬é¡"""

    def setup(self) -> None:
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆå®¹å™¨å•Ÿå‹•æ™‚é‹è¡Œä¸€æ¬¡ï¼‰"""
        print("ğŸš€ åˆå§‹åŒ– DeepSeek-OCR æ¨¡å‹...")

        model_name = "deepseek-ai/DeepSeek-OCR"

        # åŠ è¼‰ Tokenizer
        print("ğŸ“– åŠ è¼‰ Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )

        # åŠ è¼‰æ¨¡å‹
        print("ğŸ§  åŠ è¼‰æ¨¡å‹ï¼ˆé¦–æ¬¡æœƒä¸‹è¼‰ ~5-10 åˆ†é˜ï¼‰...")
        self.model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2"
        )

        # GPU è‡ªå‹•æ”¯æŒï¼ˆCog è‡ªå‹•è™•ç†ï¼‰
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print(f"âœ… æ¨¡å‹å·²åŠ è¼‰åˆ° GPU: {torch.cuda.get_device_name(0)}")
        else:
            print("âš ï¸  ä½¿ç”¨ CPUï¼ˆæœƒè¼ƒæ…¢ï¼‰")

        self.model.eval()
        print("âœ… æ¨¡å‹å·²æº–å‚™å°±ç·’ï¼\n")

    def predict(
        self,
        image_url: str = Input(
            description="Image URL, Base64, or file path (JPG/PNG/WebP/PDF)"
        ),
        prompt: str = Input(
            default="Convert the document to markdown",
            description="OCR æç¤ºè©"
        ),
    ) -> str:
        """
        OCR æ¨ç†ç«¯é»

        Args:
            image_url: åœ–åƒ URL æˆ– Base64
            prompt: æŒ‡å° OCR çš„æç¤ºè©

        Returns:
            æå–çš„æ–‡æœ¬å…§å®¹
        """
        try:
            print(f"ğŸ”„ è™•ç†è«‹æ±‚...")

            # åŠ è¼‰åœ–åƒ
            image = self._load_image(image_url)

            # ä¿å­˜ç‚ºè‡¨æ™‚æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp_file:
                image.save(tmp_file.name, "JPEG")
                image_path = tmp_file.name

            print(f"   åœ–åƒå¤§å°: {image.size}")

            # åŸ·è¡Œ OCR
            print("   é‹è¡Œ OCR æ¨ç†...")
            with torch.no_grad():
                result = self.model.infer(
                    self.tokenizer,
                    prompt=f"<image>\n{prompt}",
                    image_file=image_path,
                    base_size=1024,
                    image_size=640,
                    crop_mode=True
                )

            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            os.remove(image_path)

            print(f"âœ… OCR å®Œæˆï¼")
            return result

        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {str(e)}")
            raise

    def _load_image(self, image_input: str) -> Image.Image:
        """
        åŠ è¼‰åœ–åƒ
        æ”¯æŒï¼šURLã€Base64ã€æœ¬åœ°æ–‡ä»¶
        """
        # å¦‚æœæ˜¯ URL
        if image_input.startswith("http"):
            print("   ğŸ“¥ å¾ URL ä¸‹è¼‰åœ–åƒ...")
            response = requests.get(image_input, timeout=30)
            response.raise_for_status()
            return Image.open(BytesIO(response.content)).convert("RGB")

        # å¦‚æœæ˜¯ Base64
        elif image_input.startswith("data:"):
            print("   ğŸ” è§£ç¢¼ Base64...")
            import base64
            base64_str = image_input.split(",")
            image_bytes = base64.b64decode(base64_str)
            return Image.open(BytesIO(image_bytes)).convert("RGB")

        # å…¶ä»–æƒ…æ³è¦–ç‚ºæœ¬åœ°æ–‡ä»¶
        else:
            print("   ğŸ“‚ åŠ è¼‰æœ¬åœ°æ–‡ä»¶...")
            return Image.open(image_input).convert("RGB")
